{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbc7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.sample(n=50000, random_state=42).reset_index(drop=True)  # on ne garde que 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e426d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer texte + hashtags\n",
    "df[\"full_text\"] = (\n",
    "    df[\"text\"].fillna(\"\") + \" \" +\n",
    "    df[\"hashtags\"].fillna(\"\").str.replace(r\"[^\\w#]+\", \" \", regex=True)\n",
    ")\n",
    "#y = np.log1p(df[\"retweet_count\"].values).astype(np.float32)\n",
    "y = df[\"retweet_count\"].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cd1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features numériques et scaler\n",
    "num_cols = [\"user_verified\", \"user_statuses_count\", \"user_followers_count\", \"user_friends_count\"]\n",
    "df[\"user_verified\"] = df[\"user_verified\"].astype(int)\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(df[num_cols].values).astype(np.float32)\n",
    "\n",
    "# 2) Tokenizer maison + vocabulaire\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fc710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "counter = Counter(tok for txt in df[\"full_text\"] for tok in simple_tokenizer(txt))\n",
    "specials = [\"<pad>\",\"<unk>\"]\n",
    "itos = specials + sorted(counter.keys())\n",
    "stoi = {tok:i for i,tok in enumerate(itos)}\n",
    "pad_idx, unk_idx = stoi[\"<pad>\"], stoi[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3712c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Dataset & DataLoader\n",
    "from torch.utils.data import random_split\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, numerics, labels):\n",
    "        self.texts, self.numerics, self.labels = texts, numerics, labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, i):\n",
    "        toks = simple_tokenizer(self.texts[i]) or [\"<unk>\"]\n",
    "        token_ids = [stoi.get(t, unk_idx) for t in toks]\n",
    "        return {\n",
    "            \"tokens\":    torch.tensor(token_ids, dtype=torch.long),\n",
    "            \"numerics\":  torch.tensor(self.numerics[i], dtype=torch.float32),\n",
    "            \"label\":     torch.tensor(self.labels[i], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs = [b[\"tokens\"] for b in batch]\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    padded  = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=pad_idx)\n",
    "    nums    = torch.stack([b[\"numerics\"] for b in batch])\n",
    "    labs    = torch.stack([b[\"label\"]    for b in batch])\n",
    "    return padded, lengths, nums, labs\n",
    "\n",
    "# split 80/20\n",
    "texts = df[\"full_text\"].tolist()\n",
    "full_ds = TweetDataset(texts, X_num, y)\n",
    "n_train = int(0.8 * len(full_ds))\n",
    "n_test  = len(full_ds) - n_train\n",
    "train_ds, test_ds = random_split(full_ds, [n_train, n_test],\n",
    "                                 generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40aaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Définition du modèle hybride\n",
    "class LSTM_MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, lstm_hid=128, num_feat_dim=4):\n",
    "        super().__init__()\n",
    "        # embedding + LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_hid, batch_first=True)\n",
    "        # MLP numérique\n",
    "        self.num_mlp = nn.Sequential(\n",
    "            nn.Linear(num_feat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        # tête de fusion\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(lstm_hid + 64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, lengths, numerics):\n",
    "        emb = self.embedding(tokens)                            # (B, L, emb_dim)\n",
    "        # pack/pad pour LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lengths.cpu(), \n",
    "                                                   batch_first=True, enforce_sorted=False)\n",
    "        out_p, (h_n, _) = self.lstm(packed)\n",
    "        # h_n[-1] = dernier hidden state de la dernière couche\n",
    "        text_repr = h_n[-1]                                      # (B, lstm_hid)\n",
    "        num_repr  = self.num_mlp(numerics)                       # (B, 64)\n",
    "        x = torch.cat([text_repr, num_repr], dim=1)              # (B, lstm_hid+64)\n",
    "        return self.head(x).squeeze(1)                           # (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a39a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 — Train MAE: 134.3818\n",
      "Epoch 2/100 — Train MAE: 133.6907\n",
      "Epoch 3/100 — Train MAE: 132.0033\n",
      "Epoch 4/100 — Train MAE: 128.9764\n",
      "Epoch 5/100 — Train MAE: 124.9385\n",
      "Epoch 6/100 — Train MAE: 120.9878\n",
      "Epoch 7/100 — Train MAE: 116.2255\n",
      "Epoch 8/100 — Train MAE: 112.3328\n",
      "Epoch 9/100 — Train MAE: 106.3381\n",
      "Epoch 10/100 — Train MAE: 102.8333\n",
      "Epoch 11/100 — Train MAE: 99.0770\n",
      "Epoch 12/100 — Train MAE: 96.2867\n",
      "Epoch 13/100 — Train MAE: 93.2358\n",
      "Epoch 14/100 — Train MAE: 90.1555\n",
      "Epoch 15/100 — Train MAE: 86.1277\n",
      "Epoch 16/100 — Train MAE: 84.8720\n",
      "Epoch 17/100 — Train MAE: 82.0888\n",
      "Epoch 18/100 — Train MAE: 80.3633\n",
      "Epoch 19/100 — Train MAE: 76.2942\n",
      "Epoch 20/100 — Train MAE: 76.7045\n",
      "Epoch 21/100 — Train MAE: 75.0718\n",
      "Epoch 22/100 — Train MAE: 72.6284\n",
      "Epoch 23/100 — Train MAE: 71.5428\n",
      "Epoch 24/100 — Train MAE: 70.0765\n",
      "Epoch 25/100 — Train MAE: 69.9302\n",
      "Epoch 26/100 — Train MAE: 67.6412\n",
      "Epoch 27/100 — Train MAE: 68.2561\n",
      "Epoch 28/100 — Train MAE: 65.7613\n",
      "Epoch 29/100 — Train MAE: 67.0376\n",
      "Epoch 30/100 — Train MAE: 64.3584\n",
      "Epoch 31/100 — Train MAE: 62.8199\n",
      "Epoch 32/100 — Train MAE: 62.7731\n",
      "Epoch 33/100 — Train MAE: 61.7625\n",
      "Epoch 34/100 — Train MAE: 62.0153\n",
      "Epoch 35/100 — Train MAE: 62.7260\n",
      "Epoch 36/100 — Train MAE: 60.3842\n",
      "Epoch 37/100 — Train MAE: 61.8768\n",
      "Epoch 38/100 — Train MAE: 60.8847\n",
      "Epoch 39/100 — Train MAE: 59.5428\n",
      "Epoch 40/100 — Train MAE: 60.0015\n",
      "Epoch 41/100 — Train MAE: 59.6011\n",
      "Epoch 42/100 — Train MAE: 59.0805\n",
      "Epoch 43/100 — Train MAE: 58.6620\n",
      "Epoch 44/100 — Train MAE: 58.9508\n",
      "Epoch 45/100 — Train MAE: 59.6859\n",
      "Epoch 46/100 — Train MAE: 58.9721\n",
      "Epoch 47/100 — Train MAE: 59.9726\n",
      "Epoch 48/100 — Train MAE: 61.1958\n",
      "Epoch 49/100 — Train MAE: 59.7627\n",
      "Epoch 50/100 — Train MAE: 59.0361\n",
      "Epoch 51/100 — Train MAE: 59.3737\n",
      "Epoch 52/100 — Train MAE: 59.2870\n",
      "Epoch 53/100 — Train MAE: 57.3482\n",
      "Epoch 54/100 — Train MAE: 57.6217\n",
      "Epoch 55/100 — Train MAE: 56.8792\n",
      "Epoch 56/100 — Train MAE: 54.4631\n",
      "Epoch 57/100 — Train MAE: 55.5813\n",
      "Epoch 58/100 — Train MAE: 55.2537\n",
      "Epoch 59/100 — Train MAE: 54.7813\n",
      "Epoch 60/100 — Train MAE: 54.6884\n",
      "Epoch 61/100 — Train MAE: 54.5848\n",
      "Epoch 62/100 — Train MAE: 55.7159\n",
      "Epoch 63/100 — Train MAE: 53.9488\n",
      "Epoch 64/100 — Train MAE: 53.3319\n",
      "Epoch 65/100 — Train MAE: 53.0123\n",
      "Epoch 66/100 — Train MAE: 52.3754\n",
      "Epoch 67/100 — Train MAE: 52.6866\n",
      "Epoch 68/100 — Train MAE: 51.6476\n",
      "Epoch 69/100 — Train MAE: 51.4848\n",
      "Epoch 70/100 — Train MAE: 51.3636\n",
      "Epoch 71/100 — Train MAE: 50.7389\n",
      "Epoch 72/100 — Train MAE: 49.9861\n",
      "Epoch 73/100 — Train MAE: 50.8354\n",
      "Epoch 74/100 — Train MAE: 50.2332\n",
      "Epoch 75/100 — Train MAE: 48.9900\n",
      "Epoch 76/100 — Train MAE: 50.2050\n",
      "Epoch 77/100 — Train MAE: 50.0966\n",
      "Epoch 78/100 — Train MAE: 48.5289\n",
      "Epoch 79/100 — Train MAE: 49.5884\n",
      "Epoch 80/100 — Train MAE: 49.7884\n",
      "Epoch 81/100 — Train MAE: 49.8358\n",
      "Epoch 82/100 — Train MAE: 49.8174\n",
      "Epoch 83/100 — Train MAE: 49.3733\n",
      "Epoch 84/100 — Train MAE: 49.8674\n",
      "Epoch 85/100 — Train MAE: 49.5099\n",
      "Epoch 86/100 — Train MAE: 48.4188\n",
      "Epoch 87/100 — Train MAE: 48.2287\n",
      "Epoch 88/100 — Train MAE: 48.2460\n",
      "Epoch 89/100 — Train MAE: 47.7325\n",
      "Epoch 90/100 — Train MAE: 45.9706\n",
      "Epoch 91/100 — Train MAE: 46.8863\n",
      "Epoch 92/100 — Train MAE: 46.6743\n",
      "Epoch 93/100 — Train MAE: 45.8524\n",
      "Epoch 94/100 — Train MAE: 46.7779\n",
      "Epoch 95/100 — Train MAE: 46.2788\n",
      "Epoch 96/100 — Train MAE: 46.6570\n",
      "Epoch 97/100 — Train MAE: 48.8023\n",
      "Epoch 98/100 — Train MAE: 46.7549\n",
      "Epoch 99/100 — Train MAE: 46.1750\n",
      "Epoch 100/100 — Train MAE: 45.8749\n",
      "Final Test MAE: 126.0021\n"
     ]
    }
   ],
   "source": [
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = LSTM_MLP(len(itos)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.L1Loss()  # Utiliser MAE pour l'entraînement\n",
    "\n",
    "n_epochs  = 100\n",
    "\n",
    "# ——— Entraînement ———\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for tokens, lengths, numerics, labels in train_loader:\n",
    "        tokens, lengths = tokens.to(device), lengths.to(device)\n",
    "        numerics, labels = numerics.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(tokens, lengths, numerics)\n",
    "        loss  = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * tokens.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch}/{n_epochs} — Train MAE: {avg_train_loss:.4f}\")\n",
    "\n",
    "# ——— Évaluation finale sur test set ———\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "criterion = nn.L1Loss()  # Utiliser MAE pour l'évaluation finale\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tokens, lengths, numerics, labels in test_loader:\n",
    "        tokens, lengths = tokens.to(device), lengths.to(device)\n",
    "        numerics, labels = numerics.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(tokens, lengths, numerics)\n",
    "        total_test_loss += criterion(preds, labels).item() * tokens.size(0)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_ds)\n",
    "print(f\"Final Test MAE: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2612c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MAE: 126.0021\n"
     ]
    }
   ],
   "source": [
    "# ——— Évaluation finale sur test set ———\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "criterion = nn.L1Loss()  # Utiliser MAE pour l'évaluation finale\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tokens, lengths, numerics, labels in test_loader:\n",
    "        tokens, lengths = tokens.to(device), lengths.to(device)\n",
    "        numerics, labels = numerics.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(tokens, lengths, numerics)\n",
    "        total_test_loss += criterion(preds, labels).item() * tokens.size(0)\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_ds)\n",
    "print(f\"Final Test MAE: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3056073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.sample(n=10000, random_state=42).reset_index(drop=True)  # on ne garde que 10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40fc710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    # enlever URLs\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)\n",
    "    # enlever mentions si vous ne voulez pas les considérer\n",
    "    t = re.sub(r\"@\\w+\", \"\", t)\n",
    "    # garder hashtags (#mot) ou extraire séparément plus bas\n",
    "    # enlever ponctuation inutile\n",
    "    t = re.sub(r\"[^a-z0-9#\\s]\", \"\", t)\n",
    "    # retirer espaces redondants\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df[\"clean_text\"] += \" \" + df[\"hashtags\"].fillna(\"\").apply(lambda x: \"#\" + x if x != \"\" else \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb97a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (incl. pad & unk) : 10002\n",
      "Exemples : [('the', 2), ('to', 3), ('of', 4), ('a', 5), ('and', 6), ('in', 7), ('is', 8), ('this', 9), ('for', 10), ('covid19', 11)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Tokenisation simple\n",
    "df['tokens'] = df['clean_text'].str.lower().str.split()\n",
    "\n",
    "# 2. Comptage\n",
    "all_tokens = [tok for tokens in df['tokens'] for tok in tokens]\n",
    "freq = Counter(all_tokens)\n",
    "\n",
    "# 3. Top N tokens\n",
    "N = 10000\n",
    "most_common = freq.most_common(N)\n",
    "\n",
    "# 4. Dictionnaire\n",
    "vocab = {tok: idx+2 for idx, (tok, _) in enumerate(most_common)}\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "# 5. Vérif\n",
    "print(f\"Vocab size (incl. pad & unk) : {len(vocab)}\")\n",
    "print(\"Exemples :\", list(vocab.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21bdf148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=50):\n",
    "        self.texts  = df['clean_text'].tolist()\n",
    "        self.targets= torch.log1p(torch.tensor(df['retweet_count'].values, dtype=torch.float))\n",
    "        #self.targets = torch.tensor(df['retweet_count'].values, dtype=torch.float )\n",
    "        self.vocab  = vocab\n",
    "        self.max_len= max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        toks = self.texts[idx].split()  # ou utilisez une tokenisation plus robuste\n",
    "        idxs = [ self.vocab.get(t, self.vocab['<unk>']) for t in toks ][:self.max_len]\n",
    "        # padding\n",
    "        if len(idxs) < self.max_len:\n",
    "            idxs += [self.vocab['<pad>']] * (self.max_len - len(idxs))\n",
    "        return torch.tensor(idxs), self.targets[idx]\n",
    "\n",
    "\n",
    "# 2) Modèle Embedding + BiLSTM + régression\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=100, hid_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm      = nn.LSTM(emb_dim, hid_dim, num_layers=1,\n",
    "                                 bidirectional=True, batch_first=True, dropout=0.4)\n",
    "        self.fc1       = nn.Linear(hid_dim*2, hid_dim)\n",
    "        self.out       = nn.Linear(hid_dim, 1)\n",
    "        self.dropout   = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb, _ = self.lstm(self.embedding(x))        # [B, L, 2*hid]\n",
    "        # pooling : moyenne sur la séquence\n",
    "        pooled = emb.mean(dim=1)                     # [B, 2*hid]\n",
    "        x = torch.relu(self.fc1(self.dropout(pooled)))\n",
    "        return self.out(self.dropout(x)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae9192",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'retweet_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/TorchDA/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'retweet_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Instanciation du Dataset complet\u001b[39;00m\n\u001b[1;32m      5\u001b[0m full_ds \u001b[38;5;241m=\u001b[39m TweetDataset(df, vocab, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m \u001b[43mTweetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2) Calcul des tailles\u001b[39;00m\n\u001b[1;32m      9\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_ds)\n",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m, in \u001b[0;36mTweetDataset.__init__\u001b[0;34m(self, df, vocab, max_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, vocab, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts  \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog1p(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretweet_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#self.targets = torch.tensor(df['retweet_count'].values, dtype=torch.float )\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab  \u001b[38;5;241m=\u001b[39m vocab\n",
      "File \u001b[0;32m~/miniconda3/envs/TorchDA/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/TorchDA/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'retweet_count'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# 1) Instanciation du Dataset complet\n",
    "full_ds = TweetDataset(df, vocab, max_len=50)\n",
    "\n",
    "# 2) Calcul des tailles\n",
    "total_size = len(full_ds)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size   = int(0.1 * total_size)\n",
    "test_size  = total_size - train_size - val_size\n",
    "\n",
    "# 3) Split aléatoire et reproductible\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_ds,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# 4) Création des DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# 5) Choix du device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = LSTMRegressor(len(vocab), emb_dim=100, hid_dim=64).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "\n",
    "# 6) Boucle d'entraînement avec validation\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # -- phase train --\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_x)\n",
    "        loss  = criterion(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    train_loss /= train_size\n",
    "\n",
    "    # -- phase validation --\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x)\n",
    "            val_loss += criterion(preds, batch_y).item() * batch_x.size(0)\n",
    "    val_loss /= val_size\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs} — Train Loss: {train_loss:.4f} — Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# 7) Évaluation finale sur le test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        preds = model(batch_x)\n",
    "        test_loss += criterion(preds, batch_y).item() * batch_x.size(0)\n",
    "test_loss /= test_size\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8ef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
